% TEMPLATE for Usenix papers, specifically to meet requirements of
%  USENIX '05
% originally a template for producing IEEE-format articles using LaTeX.
%   written by Matthew Ward, CS Department, Worcester Polytechnic Institute.
% adapted by David Beazley for his excellent SWIG paper in Proceedings,
%   Tcl 96
% turned into a smartass generic template by De Clarke, with thanks to
%   both the above pioneers
% use at your own risk.  Complaints to /dev/null.
% make it two column with no page numbering, default is 10 point

% Munged by Fred Douglis <douglis@research.att.com> 10/97 to separate
% the .sty file from the LaTeX source template, so that people can
% more easily include the .sty file into an existing document.  Also
% changed to more closely follow the style guidelines as represented
% by the Word sample file. 

% Note that since 2010, USENIX does not require endnotes. If you want
% foot of page notes, don't include the endnotes package in the 
% usepackage command, below.

%----------------------------------------------------------------------------------------
%	PACKAGES & CONFIG
%----------------------------------------------------------------------------------------

%\documentclass[paper=a4, fontsize=11pt]{article} % A4 paper and 11pt font size
\documentclass[letterpaper,twocolumn,10pt]{article}

\usepackage{usenix,epsfig,endnotes}
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{listings} % source code package
\usepackage{tikz} % graph package
\usepackage{supertabular} % table span multiple pages
\usetikzlibrary{shapes,arrows}
\usepackage{courier}
\usepackage[hyphens]{url} %for showing urls in bibliography
%\usepackage{hyperref} %for breaking urls in bib
%\hypersetup{colorlinks=true,breaklinks=true} %for showing urls

%\usepackage{sectsty} % Allows customizing section commands
%\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps
%\setlength\parindent{0pt} % Removes all indentation from paragraphs

\newtheorem{theorem}{Theorem}[section]
\DeclareMathOperator*{\sign}{sgn}

%----------------------------------------------------------------------------------------
%	TITLE
%----------------------------------------------------------------------------------------
\usepackage{titling}
\newcommand{\subtitle}[1]{%
  \posttitle{%
    \par\end{center}
    \begin{center}\large#1\end{center}
    \vskip0.5em}%
}

\begin{document}

%make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf A Hopfield Network for Digits Recognition}
\subtitle{\textit{Artificial Intelligence Course Project}}


\author{
{\rm Gianluca Barbon}\\
818453@stud.unive.it
%\and
}

%\date{July 24, 2014}
\maketitle

% Use the following at camera-ready time to suppress page numbers.
% Comment it out when you first submit the paper for review.
\thispagestyle{empty}


%\subsection*{Abstract}
\begin{abstract}
\emph{This report describes the artificial intelligence course project. This project consists in the implementation of a Hopfield Network using python. The weight matrix will be computed by using different algorithms, in such a way to analyse performances and thus identify the best solution.}
\end{abstract}

%----------------------------------------------------------------------------------------
%	1. Introduction
%----------------------------------------------------------------------------------------

\section{Introduction}

\subsection{Neural Networks}
They are used for recognition and classification problem, they are capable to learn and so to generalize, that is produce outputs in correspondence of inputs never met before. The training of the net take place presenting a training set (set of example) as input. The answer given by the net for each example will be compared to the desired answer, the difference (or error) between the two will be evaluated and finally the weights will be adjusted by looking at this difference. the process is repeated for the entire training set, until the produced error is minimized, so under a preset threshold.

\subsubsection{Classification Problems}
Classification problems consists in the classification of an object by looking at its features.

\subsection{Hopfield Network \cite{pelillo}}
Hopfield networks are neural networks that can be seen as non linear dynamic systems. They are also called recurring networks or feedback networks. 
We consider neural networks as non linear dynamic systems, where we consider the time variable. In order to do this, we must take into account loops, so we will use recurrent networks.
recurrent networks with non linear units are difficult to analyse: they can converge to a stable state, oscillate or follow chaotic trajectories whose behaviour is not predictable.
However, the american physicist J.J. Hopfield discovered that with symmetric connections there will exist a stable global energy function.
The Hopfield networks have the following properties:
\begin{itemize}
\item\textbf{sigle layer recurrent networks} in which each neuron is connected to all the others, with the exception of itself (so no cycles are admitted)
\item\textbf{symmetric:} the synaptic weight matrix is symmetric, so $W=W^T$. This means that the weights is the same in both direction between two neurons.
\item\textbf{not linear:} in the continuous formulation, each neuron has a non linear invertible activation function
\end{itemize}
As for the neuron update, we can use three possible approaches:
\begin{itemize}
\item\textbf{asynchronous update:} where neurons are updated one by one
\item\textbf{synchronous update:} all neurons are updated at the same moment
\item\textbf{continuous update:} all neurons are updated in a continuous way
\end{itemize}
There exists two formulation of the Hopfield model: the discrete one and the continuous one, that differ for the way in which the time flows.
For this project we will use the discrete model. In this model the time flows in discrite way and neurons updates in asynchronous way. as for the neuron input, the McCulloch and Pitts model is used, with the adding of an external influence (or bias?) factor:
\begin{displaymath}
	H_i = \underbrace{\sum_{j \neq i} w_{ij} V_j}_\textrm{M\&P model} + \underbrace{I_i}_\textrm{external input}
\end{displaymath}
The activation function is the following:
\begin{align}
	V_i = \begin{cases}
		+1 & \text{se } H_i > 0 \\
		-1 & \text{se } H_i < 0
	\end{cases}\label{eq:learningrule}
\end{align}
The update of the neurons is a random process and the selection of the unit to be updated can be done in two ways:
\begin{enumerate}
\item at each time instant the unit to be updated is chosen randomly (this mode is useful in simulations)
\item each unit is updated independently with constant probability at each time instant
\end{enumerate}
Unlike feedforward networks, a Hopfield network is a dynamic system. It starts from an initial state
\begin{align*}
	\vec{V}(0) = (V_1(0), \dots, V_n(0))^T
\end{align*}
and evolves through a trajectory until it reach a fixed point in which $V(t+1)=V(t)$ (convergence). The Hopfield theorem supplies a sufficient condition for the convergence of the system. It uses an energy function E that govern the systems :
\begin{align}
	E = - \frac{1}{2} \sum_{i=1}^n \sum_{\substack{j=1 \\ j \neq i}}^n w_{ij} V_i V_j - \sum_{i=1}^n I_i V_i\label{eq:energy}
\end{align} 
\begin{theorem}[Hopfield theorem: discrete case]
If the weights matrix in a Hopfield network is symmetric, $\textsf{diag}(W)=0$, the Energy Function will be a Lyapunov function for the system, so: 
\begin{center}
$\Delta E = E(t+1)-E(t)\leq 0$
\end{center}
with the equivalence when the system reach a stationary point.
\end{theorem}

\subsection{Learning Algorithms}
Learning rule have some characteristics:
\begin{itemize}
\item\textbf{locality:} a rule can be local, this means that the update of a given weight depends only on informations available to neurons on either side of the connection. Locality provides natural parallelism that is one of the component that makes an Hopfield network a truly parallel machine.
\item\textbf{incremental:} an incremental rule modifies the old network configuration to memorize a new pattern without needing to refer to any of the previous learnt patterns. This behaviour allows an Hopfield net to be adaptive, thus more suitable for real time situations and changing environments
\item\textbf{immediate:} an immediate update of the network allows faster learning
\item\textbf{capacity:} it measure how many patterns can be stored in the network of a given size (number of neurons). Moreover higher capacity allows faster processing times, because the update time is at least proportional to the number of neurons. 
\end{itemize}

\subsubsection{Hebb's rule}
In contrast with computer's byte-addressable memory, that adopt a precise memory address to locate information, the human brain utilize content-addressable memory, that uses the content of data to locate information. The Hebb rule has been introduced to describe such behaviour and states that the weight between two neurons increases if the two neurons activate simultaneously. In detail, the Hebb postulate says that: 
\begin{center}
\emph{Let us assume that the persistence or repetition of a reverberatory activity (or "trace") tends to induce lasting cellular changes that add to its stability. [\ldots] When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased. }\cite{Hebb49}
\end{center}
The memory is represented like a set of $P$ patterns $x^\mu$, where $\mu = 1,\ldots,\P$: when a new pattern $x$ is presented, the net typically answers producing the pattern in memory that most resembles to $x$. Accordingly to the Hebb postulate, proportional weights are used in the activation between a pre and a post synaptic neurons:
\begin{displaymath}
	w_{ij} = \frac{1}{N} \sum_{\mu = 1}^P x_i^\mu x_j^\mu
\end{displaymath}
where $N$ is the number of binary units with output $s_1,\ldots,s_N$. The recall mechanism is the following:
\begin{displaymath}
	s_i = \sign\left(\sum_j w_{ij} s_j \right)
\end{displaymath}
Anyway, there are some problems in the use of Hopfield network as content-addressed memories:
\begin{itemize}
\item the maximum number of pattern is $0.15N$
\item sometimes the net produces spurious states, that is states that do not belong to the memorized patterns
\item the recalled pattern is not necessarily the most similar to the input one
\item patterns are not recalled with the same emphasis
\end{itemize}

\subsubsection{Pseudo-Inverse Rule}
Pseudo inverse rule try to map a memory to itself before thresholding. In this way the problem can be treated as a real value regression problem, the solution of which is given by the pseudo-inverse. We can consider the Hebb rule as a special case of pseudo-inverse rule for orthogonal training vectors. However, in this rule we have no local computation and no incremental updates, because it involves the calculation of an inverse.
\begin{displaymath}
	w_{ij} = \frac{1}{N} \sum_{\substack{u=1\\v=1    }}^m x_i^u (Q^{-1})^{uv} x_j^v
\end{displaymath}
where $m$ is the total number of patterns and $q$ is:
\begin{displaymath}
	q_{uv} = \frac{1}{N} \sum_{i=1}^N x_i^u x_j^v
\end{displaymath}
This rule, also called projection learning rule \cite{personnazcoll}, allows to improve the retrieval capability of the net with respect to Hebb rule, bringing the maximum number of pattern to $0.5N$.

\subsubsection{Storkey Rule \cite{Storkey97increasingthe}}
Even if the pseudo-inverse rule performs better than the Hebbian one, it is never incremental and local only if not immediate. Storkey propose an algorithm to increase the capacity of the Hebbian rule without losing locality and incrementality.
\begin{displaymath}
	w_{ij}^0=0\ \forall\ i,j\ \ \text{and}
\end{displaymath}
\begin{displaymath}
	w_{ij}^{\nu} = w_{ij}^{\nu-1}+ \frac{1}{N} x_i^{\nu}x_j^{\nu} - \frac{1}{N} x_i^{\nu}h_{ji}^{\nu} - \frac{1}{N} h_{ij}^{\nu}x_j^{\nu}  
\end{displaymath}
where $h_{ij}$ is a form of local field at neuron $i$:
\begin{displaymath}
	h_{ij}^{\mu} = \sum_{\substack{k=1\\k\neq i,j}}^n w_{ij}^{\mu-1} x_k^{\mu}
\end{displaymath}

\subsection{Network Capacity}

\section{Implementation}
The Hopfield network is implemented with the HopfieldNet class. Thus, a network will be an instance of this class. When an HopfieldNet object is created, the net is immediately initialized with the given input and the given training algorithm. The class is provided with a test function that performs the test of the whole net with the given test set. This function uses a neuron activation function, called $single\_unit\_updater$, that operates on a single unit. The behaviour of the net is governed by the energy function, called by the test function at each iteration. Units to be updated are chosen randomly; when every units has been updated, the energy is computed and compared to the one obtained at time $t-1$. When the difference between the two is zero, the update phase is stopped and result is returned to the user.

\subsection{Learning Algorithms}

\subsubsection{Hebbian rule}
The Hebbian rule has been implemented in three different ways. The first one is the original algorithm, with three nested loops: the outer one to iterate over training patterns, while the internal one iterate over the elements of the matrix. This solution, while perfectly follows the algorithm, does not performs wells, so it has been decided to exploits the strengths of the python language to improve performances. Following this idea, solution two uses the \emph{dot product} function provided by python in order to compute the product between points of a pattern and the sum of the corresponding results for each point between different patterns, thus avoiding the outer loop. This advantage is better exploited in solution three (the one adopted in the definitive version), where also the two loops that iterates over elements are removed. This solution perform directly the dot products between the transposed input patterns matrix and the normal one. Anyway, even if solution three is the best one, it has been discovered that solution one and two are the best choice for very small images.

\subsubsection{Pseudo-inverse rule}
The pseudo-inverse rule has not been improved with predefined python functions, because of the complexity of the formula. Thus, the pseudo inverse training algorithm is implemented with four nested loops: the external ones iterate over training patterns while the internal ones iterate over single units for the weight computation. Moreover, this algorithm exploits a separate function for the computation of the $Q$ matrix. The original algorithm that iterated the units for the sum computation has been substituted with a dot product operator thanks to numpy library. Finally, in the pseudo inverse implementation the $Q$ matrix is already returned as an inverse, in order to save computational time. 

\subsubsection{Storkey's rule}
The core Storkey formula has been implemented using loops. Improvement where not possible, because dot product could not be exploited. As for the pseudo-inverse alg. also this one uses an external function, the $h\_storkey$ function, that computes $h$. Also this function has not been optimized, even if this was possible, because the result with the dot product where too bad with respect to the normal one.\\ \\
\noindent Notice that both pseudo inverse and Storkey avoid the computation of half of the matrix, by taking advantage of the fact that the matrix is symmetric.

\subsection{Minor functions}
A lot of minor functions have been implemented in order to support the program. Dataset loading functions have been provided, as well as plotter function to show results. Two interesting functions, that resulted very useful in the test phase, are the following ones:
\subsubsection{Corruption and Erase functions}
In order to test the capacity of the net to recognize corrupted images, two function have been implemented:
\begin{itemize}
\item\textbf{corrupter} this function corrupt the images by introducing noise, represented by $1$ values. A parameter allows to decide the percentage of image to be noised. Involved points are randomly distributed in the image.
\item\textbf{image\_eraser} this function will erase (fill with "1" values) a part of the image. The percentage of the deletion is decided through a parameter.
\end{itemize} 

\section{Testing the net}

\subsection{Data set}
These section describes the dataset used in the project. Each dataset has been used for both training and testing phases, with the exception of the Semeion, which has been used only for testing.
\begin{itemize}
\item\textbf{Courier Font Digits} This dataset has been created for this project. It is composed of ten tiff images of $44x70$ pixels in RGB colormap, with white background. Each image contain a different digit written in black Courier font, from 0 to 9. These images come from $100x100$ pixels images, where lateral white spaces have been removed in order to improve algorithm efficiency.
\item\textbf{Digital 7 Font Digits} Also this dataset has been created for this project. Like the Courier data set, it is composed of ten tiff images, where each one contains a different digit written in black Digital 7 font, from 0 to 9. The image dimension is of $44x70$ pixels. As the Courier data set images are in RGB colormap, with white background. 
\item\textbf{Semeion Handwritten Data Set} This dataset was created by Tactile Srl and donated in 1994 to Semeion Research Center of Sciences of Communication for machine learning research. The data set  consists of 1593 scanned handwritten digits from around 80 persons, stretched in a rectangular box 16x16 in a gray scale of 256 values. Then each pixel of each image was scaled into a bolean (1/0) value using a fixed threshold. Each person wrote on a paper all the digits from 0 to 9, twice (the first time in the normal way while the second one in a fast way)\cite{semeion}.
\end{itemize}

\subsection{Results}
The network has been tested with the three different learning algorithms: Hebbian, Pseudo-inverse and Storkey. The most relevant tests are the ones performed with the use of the Courier Font Digits dataset, with an image dimension of $14x9$ that leads to a network of $126$ units. The Pseudo-Inverse is resulted the best learning algorithm, with an accuracy of $100\%$. The Storkey resulted second with an accuracy of $98,7\%$ while the last one is resulted the Hebbian rule, performing only an accuracy of $74,4\%$. On the contrary, as for time performances, the Hebbian resulted the best one, by training a network with 10 patterns in just $0.0002$ seconds. Pseudo-inverse rule obtained $1.5$ seconds for the same number of patterns, while the Storkey's resulted the worst algorithm, computing the training in about $21$ seconds. It is important to notice that Hebbian implementation exploits the dot product function supplied by python, while the Storkey's rule could not be improved because of its complexity.\\

Some tests have been developed to check the ability of the network to recall stored memories with the use of corrupted or partial images. Results in this case are very interesting. In example with the corruption of the $20\%$ of each test images and the deletion of the $30\%$ of the same images, the Pseudo-inverse algorithm still obtained an high accuracy, correctly matching the $89\%$ of the images. Also in this case, the Pseudo-inverse resulted the best learning algorithm.\\

Further tests have been done with the use of the Digital 7 Font Digits data set. In this case the chosen dimension is of $25x16$, leading to a network of $400$ units. The aim of this test was to check the network behaviour with larger and clearer images. Anyway, even if images in this data set appear to be clearer to a human, the network results instead increased the number of errors. This is due to the fact that this font uses digits that are very similar. In example, vertical lines are used in almost all the numbers, and this lead to an incorrect match by the Hopfield net.\\ 

Finally a set of tests has been performed also with the use of the Courier Font Digits dataset as training and the Semeion Handwritten Data Set as test set. The dimension of the training patterns has been converted to $16x16$ in order to match the Semeion one, thus bringing the total number of units to $256$. Even if such kind of test could be quite strange (the numbers used in the test are handwritten while the ones used for training come from a computer font), results are interesting: indeed, the network appears to be able to recognize some of the handwritten digits. This is also due to the fact that the Courier font used in training images fits well handwritten digits. The fact that the Courier Font Digits set images have been stretched to match the Semeion images also contributed to positive results. Finally, it is important to notice that handwritten digits are picked randomly from the dataset.\\

\subsubsection{Images filtering}
Hebbian rule results are improved by adding a filter in the image sampling before the network training. This filter is a median filter provided by the Python Image Library, and has been inserted after the conversion of the image to black and white. This kind of filter is generally used to perform noise reduction in images, and appeared to be very useful in the image file reading/conversion by filling white pixels or removing useless pixels near the images edges. Results confirmed these behaviours: with the hebbian training algorithm, there were an improvement of about $9\%$ in correct result (with respect to results without the use of the filtering).


%----------------------------------------------------------------------------------------
%	x. Conclusions
%----------------------------------------------------------------------------------------

\section{Results and Conclusions}
The Pseudo-Inverse rule algorithm resulted to be the best learning rule, even if it does not grant locality and it is not incremental. Storkey's one obtains very similar results by granting these two features, but, due to implementation constraints, it is very slow.\\
The test also shows the importance of the training images. Indeed, images that are very different between them performs better, while digits that results in similar images, even if they represents different numbers, are mismatched by the network. This behaviour affect also the capacity rule, that becomes not true for the Hebbian algorithm: even if we have a sufficient number of neurons to collect 10 patterns, the network still do not performs well.
%Increase of the number of units (and thus image dimension) does not affect results.

%\subsection{Future developments}
%\cite{pelillo} 

%----------------------------------------------------------------------------------------
%	NOTES AND BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

%{\footnotesize \bibliographystyle{acm}
\bibliographystyle{abbrv}
\bibliography{biblio}
%}

%\theendnotes

%column break
\vfill
\break

%----------------------------------------------------------------------------------------
%	APPENDICES
%----------------------------------------------------------------------------------------

\onecolumn
\appendix
\label{app:appendixB}
%\lstset{language=Python}
\lstdefinestyle{myCustomMatlabStyle}{
  language=Python,
  numbers=left,
  stepnumber=1,
  numbersep=10pt,
  tabsize=4,
  showspaces=false,
  showstringspaces=false
}  
\lstset{basicstyle=\small,style=myCustomMatlabStyle}
\section{Appendix: Code}

\subsection{HopfieldNet.py}
\begin{lstlisting}
__author__ = 'gbarbon'

import numpy as np
import random as rd
import trainers as tr


class HopfieldNet:

    def __init__(self, train_input, trainer_type, image_dimensions):

        #number of training patterns and number of units
        n_patterns = train_input.shape[0]
        self.n_units = train_input.shape[1]

        # crating threshold array (each unit has its own threshold)
        self.threshold = np.zeros(self.n_units)

        # setting image dimension
        self.image_dimensions = image_dimensions

        # net training
        if trainer_type == "hebbian":
            # self.weights = tr.hebb_train(train_input, n_patterns, self.n_units)
            self.weights = tr.hebb_train(train_input, self.n_units)
        elif trainer_type == "pseudoinv":
            self.weights = tr.pseudo_inverse_train(train_input, n_patterns, self.n_units)
        elif trainer_type == "storkey":
            self.weights = tr.storkey_train(train_input, n_patterns, self.n_units)
        elif trainer_type == "sanger":
            self.weights = tr.sanger_train(train_input, n_patterns, self.n_units)
        #else:

    def single_unit_updater(self, unit_idx, pattern):

        #temp = sum(weights[unit_idx,:]*pattern[:]) - threshold[unit_idx]
        # we implement a powerful version that uses dotproduct with numpy
        temp = np.dot(self.weights[unit_idx, :], pattern[:]) - self.threshold[unit_idx]
        if temp >= 0:
            # pattern[unit_idx] = 1
            return 1
        else:
            # pattern[unit_idx] = 0
            return -1

    def energy(self, pattern):
        e = 0
        length = len(pattern)
        for i in range(length):
            for j in range(length):
                if i == j:
                    continue
                else:
                    e += self.weights[i][j] * pattern[i] * pattern[j]

        sub_term = np.dot(self.threshold, pattern)
        e = -1 / 2 * e - sub_term
        return e

    # function overloading in threshold
    def test(self, pattern, threshold=0):

        #setting threshold if threshold != 0
        if threshold != 0:
            self.threshold = threshold

        pattern = pattern.flatten()  # flattening pattern
        energy = self.energy(pattern)  # energy init

        k = 0
        while k < 10:
            randomrange = list(range(len(pattern)))
            rd.shuffle(randomrange)
            for i in randomrange:
                pattern[i] = self.single_unit_updater(i, pattern)
                #nota: l'energia deve essere calcolata ad ogni cambiamento di una singola unita' o di tutte le unita'?
            temp_e = self.energy(pattern)
            if temp_e == energy:
                #break while loop
                pattern.shape = (self.image_dimensions[0], self.image_dimensions[1])
                return pattern
            else:
                energy = temp_e
            k += 1

        pattern.shape = (self.image_dimensions[0], self.image_dimensions[1])
        return pattern
\end{lstlisting}

\clearpage
\subsection{trainers.py}
\begin{lstlisting}
__author__ = 'gbarbon'

import numpy as np
import time


# train intput is in the form of a vector
# def hebb_train(train_input, n_patterns, n_units):
def hebb_train(train_input, n_units):
    # weights matrix init to zeros

    # # 1: original hebb algorithm
    # start = time.time()
    # w1 = np.zeros((n_units, n_units))
    # start = time.time()
    # for l in range(n_patterns):
    # for i in range(n_units):
    # for j in range(i+1, n_units): # in order to compute only the upper half matrix
    # w1[i, j] += train_input[l, i] * train_input[l, j]
    #             w1[j, i] = w1[i, j]
    # w1 *= 1 / float(n_units)
    # end = time.time()
    # elapsed = end - start
    # print("w1 elapsed time", elapsed)

    # # 2: hebb rule improved substituting the pattern loop with dot products
    # start = time.time()
    # w2 = np.zeros((n_units, n_units))
    # train_transp = zip(*train_input)
    # for i in range(n_units):
    #     for j in range(i+1, n_units): # in order to compute only the upper half matrix
    #         w2[i, j] = np.dot(train_transp[i], train_transp[j])
    #         w2[j, i] = w2[i, j]
    # w2 *= 1 / float(n_units)
    # end = time.time()
    # elapsed = end - start
    # print("w2 elapsed time", elapsed)

    # 3: hebb rule improved with matrix multiplication
    start = time.time()
    train_transp = zip(*train_input)  # matrix transpose
    train_transp = np.transpose(train_input)  # matrix transpose
    #train_transp = train_transp.astype(np.float64)
    #w3 = np.zeros((n_units, n_units))
    w3 = np.dot(train_transp, train_input)
    w3 = w3.astype(float)
    w3 *= 1 / float(n_units)
    np.fill_diagonal(w3, 0)
    end = time.time()
    elapsed = end - start
    #print("w3 elapsed time", elapsed)
    print("Hebbian elapsed time", elapsed)

    # # Checking issues
    # if np.array_equal(w1,w2) and np.array_equal(w2, w3):
    #     print("All methods returns the same weight matrix.")

    # else:
    #     print("w1 and w2 ", np.array_equal(w1,w2))
    #     print("w2 and w3 ", np.array_equal(w2,w3))
    #     print("w3 and w1 ", np.array_equal(w1,w3))

    return w3


# support function fro pseudo inverse rule
def q_pseudo_inv(train_input, n_patterns, n_units):
    # # Original version with loop
    # start = time.time()
    # q1 = np.zeros((n_patterns, n_patterns))
    # for v in range(n_patterns):
    # for u in range(n_patterns):
    # for i in range(n_units):
    #             q1[u][v] += train_input[v][i] * train_input[u][i]
    # end = time.time()
    # elapsed = end - start
    # print("original version elapsed time", elapsed)

    # New version with dot product
    # start = time.time()
    q2 = np.zeros((n_patterns, n_patterns))
    for v in range(n_patterns):
        for u in range(n_patterns):
            q2[u][v] = np.dot(train_input[v], train_input[u])
    # end = time.time()
    # elapsed = end - start
    # print("improved version elapsed time", elapsed)

    # # Checking issues
    # if np.array_equal(q1, q2):
    #     print("Both methods returns the same Q matrix.")

    q2 *= 1 / float(n_units)
    q = np.linalg.inv(q2)  # inverseof the matrix

    return q


# uses the pseudo inverse training rule
def pseudo_inverse_train(train_input, n_patterns, n_units):
    weights = np.zeros((n_units, n_units))

    start = time.time()
    # notice: the matrix is returned already inverse
    q = q_pseudo_inv(train_input, n_patterns, n_units)

    for v in range(n_patterns):
        for u in range(n_patterns):
            for i in range(n_units):
                for j in range(i + 1, n_units):  # in order to compute only the upper half matrix
                    weights[i, j] += train_input[v, i] * q[v][u] * train_input[u, j]
                    weights[j, i] = weights[i, j]
    weights *= 1 / float(n_units)

    end = time.time()
    elapsed = end - start
    print("Pseudo inverse elapsed time", elapsed)

    return weights


# support function for storkey rule
def h_storkey(weights, i_index, j_index, pattern, pattern_idx, n_units):
    h = 0

    # start = time.time()
    for k in range(n_units):
        if k != i_index and k != j_index:
            h += weights[i_index][k] * pattern[pattern_idx][k]
    # end = time.time()
    # elapsed1 = end - start

    # start = time.time()
    # h2 = 0
    # h2 = np.dot(weights[i_index], pattern[pattern_idx])
    # end = time.time()
    # h2 -= weights[i_index][i_index]*pattern[pattern_idx][i_index]
    # h2 -= weights[i_index][j_index]*pattern[pattern_idx][j_index]
    # # end = time.time()
    # elapsed2 = end - start
    #
    # if h==h2:
    # print("Results are the same")
    # else:
    # print("RESULTS ARE DIFFERENT!! h1: ", h ," h2: ", h2)
    # print("First perf is ", elapsed1," while dot is ", elapsed2)

    return h


# uses the storkey rule
def storkey_train(train_input, n_patterns, n_units):
    weights = np.zeros((n_units, n_units))

    start = time.time()
    for l in range(n_patterns):
        for i in range(n_units):
            for j in range(i + 1, n_units):  # in order to compute only the upper half matrix
                temp = train_input[l, i] * train_input[l, j]
                temp -= train_input[l, i] * h_storkey(weights, j, i, train_input, l, n_units)
                temp -= h_storkey(weights, i, j, train_input, l, n_units) * train_input[l, j]
                temp *= 1 / float(n_units)
                weights[i, j] += temp
                weights[j, i] = weights[i, j]
        print("Storkey rule iteration: ", l)
    end = time.time()
    elapsed = end - start
    print("Storkey algorithm execution time: ", elapsed)

    return weights


# sanger rule applied to hopfield networks, probably not working
def sanger_train(train_input, n_patterns, n_units):
    weights = np.zeros((n_units, n_units))

    start = time.time()
    for l in range(n_patterns):
        for i in range(n_units):
            for j in range(i + 1, n_units):
                temp = 0
                for k in range(j):
                    temp += weights[i][k] * train_input[l][k]
                weights[i][j] = train_input[l][j] * train_input[l][i] - train_input[l][j] * temp
                weights[i][j] *= 1 / float(n_units)
                weights[j][i] = weights[i][j]

    end = time.time()
    elapsed = end - start
    print("Sanger algorithm execution time: ", elapsed)

    return weights
\subsection{imageManager.py}
__author__ = 'gbarbon'

from PIL import Image
from PIL import ImageFilter
# from PIL import ImageEnhance
import os
import numpy as np


# NOTE: in images dimension are: first:= columns, second:= rows

def image_cropper(image, new_dimensions):
    width, height = image.size  # Get dimensions

    left = (width - new_dimensions[1]) / 2
    top = (height - new_dimensions[0]) / 2
    right = (width + new_dimensions[1]) / 2
    bottom = (height + new_dimensions[0]) / 2

    return image.crop((left, top, right, bottom))


def image_resizer(image, new_dimensions):
    return image.resize(new_dimensions)


def to_greyscale(image):
    return image.convert("L")


def to_blackwhite(input_image):
    return input_image.convert("1")


# convert image to a matrix of 0 and 1 and to black and white
def tomatrix_bew(image):
    imarray = np.array(image.getdata(), np.uint8).reshape(image.size[1], image.size[0])
    row = imarray.shape[0]
    cols = imarray.shape[1]
    newarray = np.zeros((row, cols))

    for i in range(row):
        for j in range(cols):
            if imarray[i][j] <= 127:
                newarray[i][j] = 1
            else:
                newarray[i][j] = 0
    return newarray


# convert image to a matrix of 0 and 1
def tomatrix(image):
    matrix = np.array(image.getdata(), np.uint8).reshape(image.size[1], image.size[0])
    rows = matrix.shape[0]
    cols = matrix.shape[1]
    newarray = np.zeros((rows, cols))

    for i in range(rows):
        for j in range(cols):
            if matrix[i][j] == 0:
                newarray[i][j] = 1

    return newarray


def collectimages(finaldim, img_dir, filter):
    i = 0
    entries = 0

    # number of files checking
    for img_file in os.listdir(img_dir):
        if img_file.endswith(".tiff"):
            entries += 1

    dataset = np.zeros((entries, finaldim[0] * finaldim[1]))

    for img_file in os.listdir(img_dir):
        if img_file.endswith(".tiff"):
            newdir = img_dir + "/" + img_file
            im = Image.open(newdir)

            orig_dim = im.size

            # Image conversion to black and white
            imp = to_greyscale(im)
            imp = to_blackwhite(imp)

            # Image filtering
            if filter == "median":
                imp = imp.filter(ImageFilter.MedianFilter(size=5))

            if orig_dim[0] > 100 and orig_dim[1] > 100:
                # crop if dimensions higher than 100
                imp = image_cropper(imp, [100, 100])
                imp = image_resizer(imp, finaldim)
            else:
                imp = image_resizer(imp, [40, 70])
                imp = image_resizer(imp, finaldim)

            #imp.show()  # shows image in external program
            imarray = tomatrix(imp)
            dataset[i] = imarray.flatten()
            i += 1

    return dataset
\end{lstlisting}

\clearpage
\subsection{utils.py}
\begin{lstlisting}
__author__ = 'gbarbon'

from matplotlib import pyplot as plt
import numpy as np
import random as rnd
import copy as cp


# convert images from 0/1 to -1/1
def image_converter(input_image):
    image = cp.copy(input_image)
    image *= 2
    image -= 1
    return image


# corrupts images
def corrupter(input_image, corr_ratio):
    dim_row = input_image.shape[0]
    dim_col = input_image.shape[1]
    total_dim = dim_col*dim_row
    corrupted_image = cp.copy(input_image).flatten()

    points_to_corr = int((total_dim*corr_ratio)/100)
    for i in range(points_to_corr):
        corr_idx = rnd.randint(0, (dim_row*dim_col-1))
        corrupted_image[corr_idx] *= -1
    corrupted_image.shape = (dim_row, dim_col)

    return corrupted_image


# erase a part of the image starting from the top
def image_eraser(input_image, erase_ratio):
    dim_row = input_image.shape[0]
    dim_col = input_image.shape[1]
    erased_img = cp.copy(input_image).flatten()

    rows_to_erase = int((dim_row*erase_ratio)/100)
    for i in range(dim_col*rows_to_erase):
        erased_img[i] = 1

    erased_img.shape = (dim_row, dim_col)

    return erased_img

#
def semeion_loader(semeion_dir, el):
    data = np.loadtxt(semeion_dir)
    n_el = data.shape[0]
    found = False
    i = rnd.randint(0, n_el)
    while ( not found):
        if data[i][256+el] == 1:
            found = True
            image = cp.copy(data[i])
        else:
            i+=1
        if i>=n_el:
            i = 0
    image = np.delete(image, [256,257,258,259,260,261,262,263,264,265,266]) # remove semeion label
    image = image_converter(image)
    image = image.reshape(16,16)
    return image


# Plot and/or save the results
def plotter(test_set, result_set, filename, plotbool, savebool):
    ntest = len(test_set)
    tickslabels_array_big = ([-0.5, 2.5, 5.5, 8.5], ['0', '3', '6', '9'], [-0.5, 2.5, 5.5, 8.5, 11.5], ['0', '3', '6', '9', '12'])
    tickslabels_array_small = ([-0.5, 4.5, 8.5], ['0', '5', '9'], [-0.5, 5.5, 11.5], ['0', '6', '12'])

    k = 1
    if (ntest>5):
        tickslabels_array = tickslabels_array_small
        lsize = 6
    else:
        tickslabels_array = tickslabels_array_big
        lsize = 8
    fig=plt.figure()
    for i in range(ntest):

        tmp = fig.add_subplot(ntest, 2, k)
        tmp.imshow(test_set[i], "summer",interpolation="nearest")

        tmp.tick_params(labelsize=lsize)
        tmp.set_xticks(tickslabels_array[0])
        tmp.set_xticklabels(tickslabels_array[1])
        tmp.set_yticks(tickslabels_array[2])
        tmp.set_yticklabels(tickslabels_array[3])
        if k==1:
            #tmp.set_title("Test set")
            tmp.text(.5, 1.2, 'Test set', horizontalalignment='center', transform=tmp.transAxes, fontsize=16)
        k += 1

        tmp = fig.add_subplot(ntest, 2, k)
        tmp.imshow(result_set[i], "winter", interpolation="nearest")
        tmp.tick_params(labelsize=lsize)
        tmp.set_xticks(tickslabels_array[0])
        tmp.set_xticklabels(tickslabels_array[1])
        tmp.set_yticks(tickslabels_array[2])
        tmp.set_yticklabels(tickslabels_array[3])
        if k==2:
            tmp.text(.5, 1.2, 'Results', horizontalalignment='center', transform=tmp.transAxes, fontsize=16)
        k += 1
    fig.subplots_adjust(hspace=.7, wspace=0.01)
    if plotbool:
        plt.show()
    if savebool:
        fig.savefig(filename, bbox_inches='tight')

\end{lstlisting}

\clearpage
\subsection{tests.py}
\begin{lstlisting}
__author__ = 'gbarbon'

import numpy as np
import HopfieldNet
import utils as utl
import imageManager as iM

# Config variables/constant
testnumber = 2
testel = 2  # elements to test
trainel = 2  # elements to train
corr_ratio = 0  # percentage of corruption ratio
erase_ratio = 0 # percentage of image erased
trainers = ["hebbian", "pseudoinv", "storkey"]
trainer = trainers[0]
filetype = "png"
all_trainer = False  # True for all trainers or False for only one
plotbool = False
savebool = True
filters = ["median", "none"]
filter = filters[0]

def test1(trainer_type):
    # corruption_val = 5
    results_dir = "/Users/jian/Dropbox/AI_dropbox/progetto_2014/results/test_1"

    # Create the training patterns
    a_pattern = np.array([[0, 0, 0, 1, 0, 0, 0],
                          [0, 0, 1, 0, 1, 0, 0],
                          [0, 1, 0, 0, 0, 1, 0],
                          [0, 1, 1, 1, 1, 1, 0],
                          [0, 1, 0, 0, 0, 1, 0],
                          [0, 1, 0, 0, 0, 1, 0],
                          [0, 1, 0, 0, 0, 1, 0]])

    b_pattern = np.array([[0, 1, 1, 1, 1, 0, 0],
                          [0, 1, 0, 0, 0, 1, 0],
                          [0, 1, 0, 0, 0, 1, 0],
                          [0, 1, 1, 1, 1, 0, 0],
                          [0, 1, 0, 0, 0, 1, 0],
                          [0, 1, 0, 0, 0, 1, 0],
                          [0, 1, 1, 1, 1, 0, 0]])

    c_pattern = np.array([[0, 1, 1, 1, 1, 1, 0],
                          [0, 1, 0, 0, 0, 0, 0],
                          [0, 1, 0, 0, 0, 0, 0],
                          [0, 1, 0, 0, 0, 0, 0],
                          [0, 1, 0, 0, 0, 0, 0],
                          [0, 1, 0, 0, 0, 0, 0],
                          [0, 1, 1, 1, 1, 1, 0]])

    a_pattern = utl.image_converter(a_pattern)
    b_pattern = utl.image_converter(b_pattern)
    c_pattern = utl.image_converter(c_pattern)

    train_input = np.array([a_pattern.flatten(), b_pattern.flatten(), c_pattern.flatten()])

    #hebbian training
    net = HopfieldNet.HopfieldNet(train_input, trainer_type, [7, 7])

    # creating test set
    a_test = utl.corrupter(a_pattern, corr_ratio)
    b_test = utl.corrupter(b_pattern, corr_ratio)
    c_test = utl.corrupter(c_pattern, corr_ratio)

    # training and testing the net
    a_result = net.test(a_test)
    b_result = net.test(b_test)
    c_result = net.test(c_test)

    #Show the results
    test_set = np.array([a_test, b_test, c_test])
    result_set = np.array([a_result, b_result, c_result])
    utl.plotter(test_set, result_set, results_dir, plotbool, savebool)


def test2(trainer_type, testel, trainel):
    images_dir = "/Users/jian/Dropbox/AI_dropbox/progetto_2014/dummy_data_set/courier_digits_data_set/tiff_images_swidth"
    results_dir = "/Users/jian/Dropbox/AI_dropbox/progetto_2014/results/test_2" + "/" + trainer_type
    filename = results_dir + "/" + filter + "_" +  "tr" + str(trainel) + "_ts"+ str(testel)  + "_c" + str(corr_ratio) + "_e" + str(erase_ratio) + "_" + trainer_type + "." + filetype
    dim = [14, 9]  # in the form rows * cols
    # testel = 8  # elements for training
    #corruption_val = 5
    # trainers = ["hebbian","pseudoinv","storkey"]

    image_dim = [dim[1], dim[0]]  # changing shape for images

    # Loading images data set
    temp_train = iM.collectimages(image_dim, images_dir, filter)

    # image conversion to 1 and -1 for Hopfield net
    for i in range(temp_train.shape[0]):
        temp = utl.image_converter(temp_train[i].reshape(dim))
        temp_train[i] = temp.flatten()

    train_input = np.zeros((trainel, dim[0] * dim[1]))
    for i in range(trainel):
        train_input[i] = temp_train[i]

    # training the net
    net = HopfieldNet.HopfieldNet(train_input, trainer_type, dim)

    # testing the net
    test_set = np.zeros((testel, dim[0], dim[1]))
    result_set = np.zeros((testel, dim[0], dim[1]))
    for i in range(testel):
        test_set[i] = temp_train[i].reshape(dim)
        if corr_ratio != 0:
            test_set[i] = utl.corrupter(test_set[i], corr_ratio)
        if erase_ratio != 0:
            test_set[i] = utl.image_eraser(test_set[i], erase_ratio)
        result_set[i] = net.test(test_set[i])

    # Plotting and saving results
    utl.plotter(test_set, result_set, filename, plotbool, savebool)


def test3(trainer_type, testel, trainel):
    images_dir = "/Users/jian/Dropbox/AI_dropbox/progetto_2014/dummy_data_set/digital7_digit_data_set/tiff_images_rawcut"
    results_dir = "/Users/jian/Dropbox/AI_dropbox/progetto_2014/results/test_3" + "/" + trainer_type
    filename = results_dir + "/" + "tr" + str(trainel) + "_ts"+ str(testel)  + "_c" + str(corr_ratio) + "_e" + str(erase_ratio) + "_" + trainer_type + "." + filetype
    dim = [25, 16]  # in the form rows * cols
    # testel = 5  # elements for training
    #corruption_val = 10

    image_dim = [dim[1], dim[0]]  # changing shape for images

    # Loading images data set
    temp_train = iM.collectimages(image_dim, images_dir, filter)

    # image conversion to 1 and -1 for Hopfield net
    for i in range(temp_train.shape[0]):
        temp = utl.image_converter(temp_train[i].reshape(dim))
        temp_train[i] = temp.flatten()

    train_input = np.zeros((trainel, dim[0] * dim[1]))
    for i in range(trainel):
        train_input[i] = temp_train[i]

    # training the net
    net = HopfieldNet.HopfieldNet(train_input, trainer_type, dim)

    # testing the net
    test_set = np.zeros((testel, dim[0], dim[1]))
    result_set = np.zeros((testel, dim[0], dim[1]))
    for i in range(testel):
        test_set[i] = train_input[i].reshape(dim)
        if corr_ratio != 0:
            test_set[i] = utl.corrupter(test_set[i], corr_ratio)
        if erase_ratio != 0:
            test_set[i] = utl.image_eraser(test_set[i], erase_ratio)
        result_set[i] = net.test(test_set[i])

    # Plotting and saving results
    utl.plotter(test_set, result_set, filename, plotbool, savebool)

def test_semeion(trainer_type, testel, trainel):
    images_dir = "/Users/jian/Dropbox/AI_dropbox/progetto_2014/dummy_data_set/courier_digits_data_set/tiff_images_swidth"
    results_dir = "/Users/jian/Dropbox/AI_dropbox/progetto_2014/results/test_semeion" + "/" + trainer_type
    semeion_dir = "/Users/jian/Dropbox/AI_dropbox/progetto_2014/semeion_data_set/semeion.data"
    filename = results_dir + "/" + "tr" + str(trainel) + "_ts"+ str(testel)  + "_c" + str(corr_ratio) + "_e" + str(erase_ratio) + "_" + trainer_type + "." + filetype
    dim = [16, 16]

    image_dim = [dim[1], dim[0]]  # changing shape for images

    # Loading images data set
    temp_train = iM.collectimages(image_dim, images_dir, filter)

    # image conversion to 1 and -1 for Hopfield net
    for i in range(temp_train.shape[0]):
        temp = utl.image_converter(temp_train[i].reshape(dim))
        temp_train[i] = temp.flatten()

    train_input = np.zeros((trainel, dim[0] * dim[1]))
    for i in range(trainel):
        train_input[i] = temp_train[i]

    # training the net
    net = HopfieldNet.HopfieldNet(train_input, trainer_type, dim)

    # loading semeion data set
    test_set = np.zeros((testel, dim[0], dim[1]))
    for i in range(testel):
        test_set[i] = utl.semeion_loader(semeion_dir, i)

    # testing the net
    result_set = np.zeros((testel, dim[0], dim[1]))
    for i in range(testel):
        # test_set[i] = temp_train[i].reshape(dim)
        if corr_ratio != 0:
            test_set[i] = utl.corrupter(test_set[i], corr_ratio)
        if erase_ratio != 0:
            test_set[i] = utl.image_eraser(test_set[i], erase_ratio)
        result_set[i] = net.test(test_set[i])

    # Plotting and saving results
    utl.plotter(test_set, result_set, filename, plotbool, savebool)


    # loading semeion data set for training



def main():
    if all_trainer:
        iterator = trainers
    else:
        iterator = [trainer]
    for i in range(len(iterator)):
        print("Now trainer is: ", iterator[i])
        if testnumber == 1:
            test1(iterator[i])
        elif testnumber == 2:
            test2(iterator[i], testel, trainel)
        elif testnumber == 3:
            test3(iterator[i], testel, trainel)
        elif testnumber == 4:
            test_semeion(iterator[i], testel, trainel)
    #total2()
    #filter_hebbian_2()

def total2():
    test_couples = [[2,2],[2,3],[3,2],[5,5],[5,10],[8,8],[10,5],[10,10]]
    for i in range(len(trainers)):
        for j in range(len(test_couples)):
            testel = test_couples[j][1]
            trainel = test_couples[j][0]
            test2(trainers[i], testel, trainel)

# remember to set hebbian as trainer before executing
def filter_hebbian_2():

    test_couples = [[2,2],[3,3],[4,4],[5,5],[6,6],[7,7],[8,8],[9,9],[10,10]]

    for j in range(len(test_couples)):
        testel = test_couples[j][1]
        trainel = test_couples[j][0]
        test2(trainer, testel, trainel)

if __name__ == "__main__":
    main()
\end{lstlisting}

%\label{app:appendixB}
%\section{Appendix:}


%----------------------------------------------------------------------------------------
%	END OF DOCUMENT
%----------------------------------------------------------------------------------------

\end{document}







