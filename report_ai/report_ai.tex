% TEMPLATE for Usenix papers, specifically to meet requirements of
%  USENIX '05
% originally a template for producing IEEE-format articles using LaTeX.
%   written by Matthew Ward, CS Department, Worcester Polytechnic Institute.
% adapted by David Beazley for his excellent SWIG paper in Proceedings,
%   Tcl 96
% turned into a smartass generic template by De Clarke, with thanks to
%   both the above pioneers
% use at your own risk.  Complaints to /dev/null.
% make it two column with no page numbering, default is 10 point

% Munged by Fred Douglis <douglis@research.att.com> 10/97 to separate
% the .sty file from the LaTeX source template, so that people can
% more easily include the .sty file into an existing document.  Also
% changed to more closely follow the style guidelines as represented
% by the Word sample file. 

% Note that since 2010, USENIX does not require endnotes. If you want
% foot of page notes, don't include the endnotes package in the 
% usepackage command, below.

%----------------------------------------------------------------------------------------
%	PACKAGES & CONFIG
%----------------------------------------------------------------------------------------

%\documentclass[paper=a4, fontsize=11pt]{article} % A4 paper and 11pt font size
\documentclass[letterpaper,twocolumn,10pt]{article}

\usepackage{usenix,epsfig,endnotes}
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{listings} % source code package
\usepackage{tikz} % graph package
\usepackage{supertabular} % table span multiple pages
\usetikzlibrary{shapes,arrows}
\usepackage{courier}
\usepackage[hyphens]{url} %for showing urls in bibliography
%\usepackage{hyperref} %for breaking urls in bib
%\hypersetup{colorlinks=true,breaklinks=true} %for showing urls

%\usepackage{sectsty} % Allows customizing section commands
%\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps
%\setlength\parindent{0pt} % Removes all indentation from paragraphs

\newtheorem{theorem}{Theorem}[section]
\DeclareMathOperator*{\sign}{sgn}

%----------------------------------------------------------------------------------------
%	TITLE
%----------------------------------------------------------------------------------------
\usepackage{titling}
\newcommand{\subtitle}[1]{%
  \posttitle{%
    \par\end{center}
    \begin{center}\large#1\end{center}
    \vskip0.5em}%
}

\begin{document}

%make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf An Hopfield Network for Digit Recognition}
\subtitle{\textit{Artificial Intelligence Course Project}}


\author{
{\rm Gianluca Barbon}\\
818453@stud.unive.it
%\and
}

%\date{July 24, 2014}
\maketitle

% Use the following at camera-ready time to suppress page numbers.
% Comment it out when you first submit the paper for review.
\thispagestyle{empty}


%\subsection*{Abstract}
\begin{abstract}
\emph{This report describes the artificial intelligence course project. This project consists in the implementation of a Hopfield Network using python. The weight matrix will be computed by using different algorithms, in such a way to analyse performances and thus identify the best solution.}
\end{abstract}

%----------------------------------------------------------------------------------------
%	1. Introduction
%----------------------------------------------------------------------------------------

\section{Introduction}

\subsection{Neural Networks}
They are used for recognition and classification problem, they are capable to learn and so to generalize, that is produce outputs in correspondence of inputs never met before. The training of the net take place presenting a training set (set of example) as input. The answer given by the net for each example will be compared to the desired answer, the difference (or error) between the two will be evaluated and finally the weights will be adjusted by looking at this difference. the process is repeated for the entire training set, until the produced error is minimized, so under a preset threshold.

\subsubsection{Classification Problems}
Classification problems consists in the classification of an object by looking at its features.

\subsection{Hopfield Network \cite{pelillo}}
Hopfield networks are neural networks that can be seen as non linear dynamic systems. They are also called recurring networks or feedback networks. 
We consider neural networks as non linear dynamic systems, where we consider the time variable. In order to do this, we must take into account loops, so we will use recurrent networks.
recurrent networks with non linear units are difficult to analyse: they can converge to a stable state, oscillate or follow chaotic trajectories whose behaviour is not predictable.
However, the american physicist J.J. Hopfield discovered that with symmetric connections there will exist a stable global energy function.
The Hopfield networks have the following properties:
\begin{itemize}
\item\textbf{sigle layer recurrent networks} in which each neuron is connected to all the others, with the exception of itself (so no cycles are admitted)
\item\textbf{symmetric:} the synaptic weight matrix is symmetric, so $W=W^T$. This means that the weights is the same in both direction between two neurons.
\item\textbf{not linear:} in the continuous formulation, each neuron has a non linear invertible activation function
\end{itemize}
As for the neuron update, we can use three possible approaches:
\begin{itemize}
\item\textbf{asynchronous update:} where neurons are updated one by one
\item\textbf{synchronous update:} all neurons are updated at the same moment
\item\textbf{continuous update:} all neurons are updated in a continuous way
\end{itemize}
There exists two formulation of the Hopfield model: the discrete one and the continuous one, that differ for the way in which the time flows.
For this project we will use the discrete model. In this model the time flows in discrite way and neurons updates in asynchronous way. as for the neuron input, the McCulloch and Pitts model is used, with the adding of an external influence (or bias?) factor:
\begin{displaymath}
	H_i = \underbrace{\sum_{j \neq i} w_{ij} V_j}_\textrm{M\&P model} + \underbrace{I_i}_\textrm{external input}
\end{displaymath}
The activation function is the following:
\begin{align}
	V_i = \begin{cases}
		+1 & \text{se } H_i > 0 \\
		-1 & \text{se } H_i < 0
	\end{cases}\label{eq:learningrule}
\end{align}
The update of the neurons is a random process and the selection of the unit to be updated can be done in two ways:
\begin{enumerate}
\item at each time instant the unit to be updated is chosen randomly (this mode is useful in simulations)
\item each unit is updated independently with constant probability at each time instant
\end{enumerate}
Unlike feedforward networks, a Hopfield network is a dynamic system. It starts from an initial state
\begin{align*}
	\vec{V}(0) = (V_1(0), \dots, V_n(0))^T
\end{align*}
and evolves through a trajectory until it reach a fixed point in which $V(t+1)=V(t)$ (convergence). The Hopfield theorem supplies a sufficient condition for the convergence of the system. It uses an energy function E that govern the systems :
\begin{align}
	E = - \frac{1}{2} \sum_{i=1}^n \sum_{\substack{j=1 \\ j \neq i}}^n w_{ij} V_i V_j - \sum_{i=1}^n I_i V_i\label{eq:energy}
\end{align} 
\begin{theorem}[Hopfield theorem: discrete case]
If the weights matrix in a Hopfield network is symmetric, $\textsf{diag}(W)=0$, the Energy Function will be a Lyapunov function for the system, so: 
\begin{center}
$\Delta E = E(t+1)-E(t)\leq 0$
\end{center}
with the equivalence when the system reach a stationary point.
\end{theorem}

\subsection{Learning Algorithms}
Learning rule have some characteristics:
\begin{itemize}
\item\textbf{locality:} a rule can be local, this means that the update of a given weight depends only on informations available to neurons on either side of the connection. Locality provides natural parallelism that is one of the component that makes an Hopfield network a truly parallel machine.
\item\textbf{incremental:} an incremental rule modifies the old network configuration to memorize a new pattern without needing to refer to any of the previous learnt patterns. This behaviour allows an Hopfield net to be adaptive, thus more suitable for real time situations and changing environments
\item\textbf{immediate:} an immediate update of the network allows faster learning
\item\textbf{capacity:} it measure how many patterns can be stored in the network of a given size (number of neurons). Moreover higher capacity allows faster processing times, because the update time is at least proportional to the number of neurons. 
\end{itemize}

\subsubsection{Hebb's rule}
In contrast with computer's byte-addressable memory, that adopt a precise memory address to locate information, the human brain utilize content-addressable memory, that uses the content of data to locate information. The Hebb rule has been introduced to describe such behaviour and states that the weight between two neurons increases if the two neurons activate simultaneously. In detail, the Hebb postulate says that: 
\begin{center}
\emph{Let us assume that the persistence or repetition of a reverberatory activity (or "trace") tends to induce lasting cellular changes that add to its stability. [\ldots] When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased. }\cite{Hebb49}
\end{center}
The memory is represented like a set of $P$ patterns $x^\mu$, where $\mu = 1,\ldots,\P$: when a new pattern $x$ is presented, the net typically answers producing the pattern in memory that most resembles to $x$. Accordingly to the Hebb postulate, proportional weights are used in the activation between a pre and a post synaptic neurons:
\begin{displaymath}
	w_{ij} = \frac{1}{N} \sum_{\mu = 1}^P x_i^\mu x_j^\mu
\end{displaymath}
where $N$ is the number of binary units with output $s_1,\ldots,s_N$. The recall mechanism is the following:
\begin{displaymath}
	s_i = \sign\left(\sum_j w_{ij} s_j \right)
\end{displaymath}
Anyway, there are some problems in the use of Hopfield network as content-addressed memories:
\begin{itemize}
\item the maximum number of pattern is $0.15N$
\item sometimes the net produces spurious states, that is states that do not belong to the memorized patterns
\item the recalled pattern is not necessarily the most similar to the input one
\item patterns are not recalled with the same emphasis
\end{itemize}

\subsubsection{Pseudo-Inverse Rule}
Pseudo inverse rule try to map a memory to itself before thresholding. In this way the problem can be treated as a real value regression problem, the solution of which is given by the pseudo-inverse. We can consider the Hebb rule as a special case of pseudo-inverse rule for orthogonal training vectors. However, in this rule we have no local computation and no incremental updates, because it involves the calculation of an inverse.
\begin{displaymath}
	w_{ij} = \frac{1}{N} \sum_{\substack{u=1\\v=1    }}^m x_i^u (Q^{-1})^{uv} x_j^v
\end{displaymath}
where $m$ is the total number of patterns and $q$ is:
\begin{displaymath}
	q_{uv} = \frac{1}{N} \sum_{i=1}^N x_i^u x_j^v
\end{displaymath}
This rule, also called projection learning rule \cite{personnazcoll}, allows to improve the retrieval capability of the net with respect to Hebb rule, bringing the maximum number of pattern to $0.5N$.

\subsubsection{Storkey Rule \cite{Storkey97increasingthe}}
Even if the pseudo-inverse rule performs better than the Hebbian one, it is never incremental and local only if not immediate. Storkey propose an algorithm to increase the capacity of the Hebbian rule without losing locality and incrementality.
\begin{displaymath}
	w_{ij}^0=0\ \forall\ i,j\ \ \text{and}
\end{displaymath}
\begin{displaymath}
	w_{ij}^{\nu} = w_{ij}^{\nu-1}+ \frac{1}{N} x_i^{\nu}x_j^{\nu} - \frac{1}{N} x_i^{\nu}h_{ji}^{\nu} - \frac{1}{N} h_{ij}^{\nu}x_j^{\nu}  
\end{displaymath}
where $h_{ij}$ is a form of local field at neuron $i$:
\begin{displaymath}
	h_{ij}^{\mu} = \sum_{\substack{k=1\\k\neq i,j}}^n w_{ij}^{\mu-1} x_k^{\mu}
\end{displaymath}

\section{Implementation}

\subsection{Learning Algorithms}

In the pseudo inverse implementation the $Q$ matrix is already returned as an inverse, in order to save computational time. The original algorithm that iterated the units for the sum computation has been substituted with a dot product operator thanks to numpy library.

Storkey learning rule. The sub function that compute $c$ has not been optimized, because the result with the dot product where too bad with respect to the normal one. As for the core Storkey formula, it has been implemented using loops. Improvement where not possible, because dot product could not be exploited.

\section{Results}

%----------------------------------------------------------------------------------------
%	x. Conclusions
%----------------------------------------------------------------------------------------

\section{Conclusions}

%\subsection{Future developments}
%\cite{pelillo} 

%----------------------------------------------------------------------------------------
%	NOTES AND BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

%{\footnotesize \bibliographystyle{acm}
\bibliographystyle{abbrv}
\bibliography{biblio}
%}

%\theendnotes

%column break
\vfill
\break

%----------------------------------------------------------------------------------------
%	APPENDICES
%----------------------------------------------------------------------------------------

\onecolumn
\appendix
\label{app:appendixA}
%\lstset{language=Python}
\lstdefinestyle{myCustomMatlabStyle}{
  language=Python,
  numbers=left,
  stepnumber=1,
  numbersep=10pt,
  tabsize=4,
  showspaces=false,
  showstringspaces=false
}  
\lstset{basicstyle=\small,style=myCustomMatlabStyle}
\section{Appendix: Code}

\subsection{HopfieldNet.py}
\begin{lstlisting}
__author__ = 'gbarbon'

import numpy as np
import random as rd
import trainers as tr


class HopfieldNet:

    def __init__(self, train_input, trainer_type, image_dimensions):

        #number of training patterns and number of units
        n_patterns = train_input.shape[0]
        self.n_units = train_input.shape[1]

        # crating threshold array (each unit has its own threshold)
        self.threshold = np.zeros(self.n_units)

        # setting image dimension
        self.image_dimensions = image_dimensions

        # net training
        if trainer_type == "hebbian":
            self.weights = tr.hebb_train(train_input, n_patterns, self.n_units)
        elif trainer_type == "pseudoinv":
            self.weights = tr.pseudo_inverse_train(train_input, n_patterns, self.n_units)
        #else:

    def single_unit_updater(self, unit_idx, pattern):

        #temp = sum(weights[unit_idx,:]*pattern[:]) - threshold[unit_idx]
        # we implement a powerful version that uses dotproduct with numpy
        temp = np.dot(self.weights[unit_idx, :], pattern[:]) - self.threshold[unit_idx]
        if temp >= 0:
            # pattern[unit_idx] = 1
            return 1
        else:
            # pattern[unit_idx] = 0
            return -1

    def energy(self, pattern):
        e = 0
        length = len(pattern)
        for i in range(length):
            for j in range(length):
                if i == j:
                    continue
                else:
                    e += self.weights[i][j] * pattern[i] * pattern[j]

        sub_term = np.dot(self.threshold, pattern)
        e = -1 / 2 * e - sub_term
        return e

    # function overloading in threshold
    def test(self, pattern, threshold=0):

        #setting threshold if threshold != 0
        if threshold != 0:
            self.threshold = threshold

        pattern = pattern.flatten()  # flattening pattern
        energy = self.energy(pattern)  # energy init

        k = 0
        while k < 10:
            randomrange = range(len(pattern))
            rd.shuffle(randomrange)
            for i in randomrange:
                pattern[i] = self.single_unit_updater(i, pattern)
                #nota: l'energia deve essere calcolata ad ogni cambiamento di una singola unita' o di tutte le unita'?
            temp_e = self.energy(pattern)
            if temp_e == energy:
                #break while loop
                pattern.shape = (self.image_dimensions[0], self.image_dimensions[1])
                return pattern
            else:
                energy = temp_e
            k += 1

        pattern.shape = (self.image_dimensions[0], self.image_dimensions[1])
        return pattern
\end{lstlisting}

\clearpage
\subsection{trainers.py}
\begin{lstlisting}
__author__ = 'gbarbon'

import numpy as np


# train intput is in the form of a vector
def hebb_train(train_input, n_patterns, n_units):
    # weights matrix init to zeros
    weights = np.zeros((n_units, n_units))

    # 1
    for l in range(n_patterns):
        for i in range(n_units):
            for j in range(n_units):
                if i == j:
                    continue
            # weights[i,j] = (1/n_units)* sum(...)
                else:
                    weights[i, j] += train_input[l, i] * train_input[l, j]
                # print("Temp weight at this point is", weights[i,j])
                # uguale???: weights[i,j] = sum(train_input[:,i]*train_input[:,j])

    return weights

#  uses the storkey rule
def storkey_train(train_input, n_patterns, n_units):
    weights = np.zeros((n_units, n_units))

    #..#

    return weights

def q_pseudo_inv(train_input, n_patterns, n_units):
    q = np.zeros((n_patterns, n_patterns))

    for v in range(n_patterns):
        for u in range(n_patterns):
            # for i in range(n_units):
            #     q[u][v] += train_input[v][i]*train_input[u][i]
            q[u][v] = np.dot(train_input[v], train_input[u])

    q *= 1 / float(n_units)
    q = np.linalg.inv(q) # inverseof the matrix

    return q

# uses the pseudo inverse training rule
def pseudo_inverse_train(train_input, n_patterns, n_units):
    weights = np.zeros((n_units, n_units))

    q = q_pseudo_inv(train_input, n_patterns, n_units)

    for v in range(n_patterns):
        for u in range(n_patterns):
            for i in range(n_units):
                for j in range(n_units):
                    if i == j:
                        continue
                    else:
                        weights[i, j] += train_input[v, i] * q[v][u] *train_input[u, j]


    weights *= 1 / float(n_units)
    return weights
\end{lstlisting}

\clearpage
\subsection{imageManager.py}
\begin{lstlisting}
__author__ = 'gbarbon'

from PIL import Image
from PIL import ImageFilter
# from PIL import ImageEnhance
import os
import numpy as np


# NOTE: in images dimension are: first:= columns, second:= rows

def image_cropper(image, new_dimensions):
    width, height = image.size  # Get dimensions

    left = (width - new_dimensions[1]) / 2
    top = (height - new_dimensions[0]) / 2
    right = (width + new_dimensions[1]) / 2
    bottom = (height + new_dimensions[0]) / 2

    return image.crop((left, top, right, bottom))


def image_resizer(image, new_dimensions):
    return image.resize(new_dimensions)


def to_greyscale(image):
    return image.convert("L")


def to_blackwhite(input_image):
    return input_image.convert("1")


# convert image to a matrix of 0 and 1 and to black and white
def tomatrix_bew(image):
    imarray = np.array(image.getdata(), np.uint8).reshape(image.size[1], image.size[0])
    row = imarray.shape[0]
    cols = imarray.shape[1]
    newarray = np.zeros((row, cols))

    for i in range(row):
        for j in range(cols):
            if imarray[i][j] <= 127:
                newarray[i][j] = 1
            else:
                newarray[i][j] = 0
    return newarray


# convert image to a matrix of 0 and 1
def tomatrix(image):
    matrix = np.array(image.getdata(), np.uint8).reshape(image.size[1], image.size[0])
    rows = matrix.shape[0]
    cols = matrix.shape[1]
    newarray = np.zeros((rows, cols))

    for i in range(rows):
        for j in range(cols):
            if matrix[i][j] == 0:
                newarray[i][j] = 1

    return newarray


def collectimages(finaldim, img_dir):
    i = 0
    entries = 0

    # number of files checking
    for img_file in os.listdir(img_dir):
        if img_file.endswith(".tiff"):
            entries += 1

    dataset = np.zeros((entries, finaldim[0] * finaldim[1]))

    for img_file in os.listdir(img_dir):
        if img_file.endswith(".tiff"):
            newdir = img_dir + "/" + img_file
            im = Image.open(newdir)

            orig_dim = im.size

            # Image conversion to black and white
            imp = to_greyscale(im)
            imp = to_blackwhite(imp)

            # Image filtering
            imp = imp.filter(ImageFilter.MedianFilter(size=5))
            #imp = imp.filter(ImageFilter.ModeFilter(size=5))

            if orig_dim[0] > 100 and orig_dim[1] > 100:
                # crop if dimensions higher than 100
                imp = image_cropper(imp, [100, 100])
                imp = image_resizer(imp, finaldim)
            else:
                imp = image_resizer(imp, [40, 70])
                imp = image_resizer(imp, finaldim)

            #imp.show()  # shows image in external program
            imarray = tomatrix(imp)
            dataset[i] = imarray.flatten()
            i += 1

    return dataset
\end{lstlisting}

\clearpage
\subsection{utils.py}
\begin{lstlisting}
__author__ = 'gbarbon'

from matplotlib import pyplot as plt
import random as rnd
import copy as cp


# convert images from 0/1 to -1/1
def image_converter(input_image):
    image = cp.copy(input_image)
    image *= 2
    image -= 1
    return image


# corrupts images
def corrupter(input_image, corrupt_param):
    dim_row = input_image.shape[0]
    dim_col = input_image.shape[1]
    corrupted_image = cp.copy(input_image).flatten()

    for i in range(corrupt_param):
        corr_idx = rnd.randint(0, (dim_row*dim_col-1))
        corrupted_image[corr_idx] *= -1
    corrupted_image.shape = (dim_row, dim_col)

    return corrupted_image


# Plot the results
def plotter(test_set, result_set):
    ntest = len(test_set)
    k = 1
    for i in range(ntest):
        plt.subplot(ntest, 2, k)
        plt.imshow(test_set[i], interpolation="nearest")
        k += 1
        plt.subplot(ntest, 2, k)
        plt.imshow(result_set[i], interpolation="nearest")
        k += 1
    plt.show()
\end{lstlisting}

\clearpage
\subsection{tests.py}
\begin{lstlisting}
__author__ = 'gbarbon'

import numpy as np
import HopfieldNet
import utils as utl
import imageManager as iM


def test1():
    # Create the training patterns
    a_pattern = np.array([[0, 0, 0, 1, 0, 0, 0],
                          [0, 0, 1, 0, 1, 0, 0],
                          [0, 1, 0, 0, 0, 1, 0],
                          [0, 1, 1, 1, 1, 1, 0],
                          [0, 1, 0, 0, 0, 1, 0],
                          [0, 1, 0, 0, 0, 1, 0],
                          [0, 1, 0, 0, 0, 1, 0]])

    b_pattern = np.array([[0, 1, 1, 1, 1, 0, 0],
                          [0, 1, 0, 0, 0, 1, 0],
                          [0, 1, 0, 0, 0, 1, 0],
                          [0, 1, 1, 1, 1, 0, 0],
                          [0, 1, 0, 0, 0, 1, 0],
                          [0, 1, 0, 0, 0, 1, 0],
                          [0, 1, 1, 1, 1, 0, 0]])

    c_pattern = np.array([[0, 1, 1, 1, 1, 1, 0],
                          [0, 1, 0, 0, 0, 0, 0],
                          [0, 1, 0, 0, 0, 0, 0],
                          [0, 1, 0, 0, 0, 0, 0],
                          [0, 1, 0, 0, 0, 0, 0],
                          [0, 1, 0, 0, 0, 0, 0],
                          [0, 1, 1, 1, 1, 1, 0]])

    a_pattern = utl.image_converter(a_pattern)
    b_pattern = utl.image_converter(b_pattern)
    c_pattern = utl.image_converter(c_pattern)

    train_input = np.array([a_pattern.flatten(), b_pattern.flatten(), c_pattern.flatten()])

    #hebbian training
    net = HopfieldNet.HopfieldNet(train_input, "hebbian", [7, 7])

    # creating test set
    a_test = utl.corrupter(a_pattern, 5)
    b_test = utl.corrupter(b_pattern, 5)
    c_test = utl.corrupter(c_pattern, 5)

    # training and testing the net
    a_result = net.test(a_test)
    b_result = net.test(b_test)
    c_result = net.test(c_test)

    #Show the results
    test_set = np.array([a_test, b_test, c_test])
    result_set = np.array([a_result, b_result, c_result])
    utl.plotter(test_set, result_set)


def test2():
    images_dir = "/Users/jian/Dropbox/AI_dropbox/progetto_2014/dummy_data_set/courier_digits_data_set/tiff_images_swidth"
    dim = [14, 9]  # in the form rows * cols
    testel = 8  # elements for training
    corruption_val = 30

    image_dim = [dim[1], dim[0]]  # changing shape for images

    # Loading images data set
    temp_train = iM.collectimages(image_dim, images_dir)

    train_input = np.zeros((testel, dim[0] * dim[1]))
    for i in range(testel):
        train_input[i] = temp_train[i]

    # image conversion to 1 and -1 for Hopfield net
    for i in range(train_input.shape[0]):
        temp = utl.image_converter(train_input[i].reshape(dim))
        train_input[i] = temp.flatten()

    # training the net
    net = HopfieldNet.HopfieldNet(train_input, "hebbian", dim)

    # testing the net
    test_set = np.zeros((testel, dim[0], dim[1]))
    result_set = np.zeros((testel, dim[0], dim[1]))
    for i in range(testel):
        test_set[i] = utl.corrupter(train_input[i].reshape(dim), corruption_val)
        result_set[i] = net.test(test_set[i])

    # Plotting results
    utl.plotter(test_set, result_set)

def test3():
    images_dir = "/Users/jian/Dropbox/AI_dropbox/progetto_2014/dummy_data_set/digital7_digit_data_set/tiff_images_rawcut"
    dim = [25, 16]  # in the form rows * cols
    testel = 10  # elements for training
    corruption_val = 10

    image_dim = [dim[1], dim[0]]  # changing shape for images

    # Loading images data set
    temp_train = iM.collectimages(image_dim, images_dir)

    train_input = np.zeros((testel, dim[0] * dim[1]))
    for i in range(testel):
        train_input[i] = temp_train[i]

    # image conversion to 1 and -1 for Hopfield net
    for i in range(train_input.shape[0]):
        temp = utl.image_converter(train_input[i].reshape(dim))
        train_input[i] = temp.flatten()

    # training the net
    net = HopfieldNet.HopfieldNet(train_input, "pseudoinv", dim)

    # testing the net
    test_set = np.zeros((testel, dim[0], dim[1]))
    result_set = np.zeros((testel, dim[0], dim[1]))
    for i in range(testel):
        test_set[i] = utl.corrupter(train_input[i].reshape(dim), corruption_val)
        result_set[i] = net.test(test_set[i])

    # Plotting results
    utl.plotter(test_set, result_set)

test3()
\end{lstlisting}

%\label{app:appendixB}
%\section{Appendix:}


%----------------------------------------------------------------------------------------
%	END OF DOCUMENT
%----------------------------------------------------------------------------------------

\end{document}







